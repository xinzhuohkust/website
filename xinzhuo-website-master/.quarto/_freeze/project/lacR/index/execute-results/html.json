{
  "hash": "ae6f9d6f7ae1b7167802b4e65604fc7d",
  "result": {
    "markdown": "---\ntitle: \"lacR\"\ndate: \"Oct 20, 2023\"\ncategories: [R Package, LAC]\ndescription: \"A Chinese tokenizer based on Baidu LAC (Lexical Analysis of Chinese)\"\ncode-fold: false\nfeed: true\ncap-location: bottom\n---\n\n```{=html}\n<style>\nbody {text-align: justify}\n</style>\n```\n\n\n:::: {.grid}\n::: {.g-col-12 .g-col-md-8 style=\"text-align: left;\"}\n## Overview {.unnumbered .unlisted}\n Compared to other Chinese word segmentation schemes, LAC performs rather well in entity information extraction, particularly for personal names and place names.\n:::\n::: {.g-col-15 .g-col-md-4 style=\"text-align: center;\"}\n<br>\n<br>\n<img src=\"lacR.png\" width=\"180\">        \n:::\n::::\n\n## Note\n\nThis package will automatically create a conda or python virtual environment. This may encounter bugs in RStudio. If you do not want to install it, you can directly use the following code after using `reticulate`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreticulate::use_python(\"'C:\\\\Users\\\\xhuangcb\\\\anaconda3\\\\envs\\\\spacy\\\\python.exe'\") # your python location\n\nLAC <- reticulate::import(\"LAC\")\n\nlac_seg <- LAC$LAC(mode = \"seg\")\n\nlac_seg$load_customization(\"custom.txt\") # optional\n\nlac_analysis <- LAC$LAC(mode = \"lac\")\n\ntokenizer <- \\(string, analysis = FALSE, progress = TRUE, min = 1) {\n    if (progress == TRUE) {\n        bar <- list(\n            format = \"Processing: {cli::pb_current}  {cli::pb_bar} {cli::pb_percent}  Rate: {cli::pb_rate}  ETA: {cli::pb_eta}\"\n        )\n    } else {\n        bar <- FALSE\n    }\n\n    if (analysis == FALSE) {\n        map(\n            string,\n            \\(x) {\n                if (!is.na(nchar(x))) {\n                    if (nchar(x) > 1) {\n                        tokens <- lac_seg$run(x)\n                        tokens <- tokens[nchar(tokens) > min]\n                        return(tokens)\n                    }\n                }\n            },\n            .progress = bar\n        )\n    } else {\n        map(\n            string,\n            \\(x) {\n                if (!is.na(nchar(x))) {\n                    if (nchar(x) > 1) {\n                        tokens <- lac_analysis$run(x)\n                        names(tokens[[1]]) <- tokens[[2]]\n                        tokens[[1]] <- tokens[[1]][nchar(tokens[[1]]) > min]\n                        return(tokens[[1]])\n                    }\n                }\n            },\n            .progress = bar\n        )\n    }\n}\n\ndf %>%\n    mutate(\n        words = str_remove_all(contents, \"\\\\p{P}|\\\\s+\") |> tokenizer(analysis = TRUE, min = 1)\n    ) # how to call tokenizer in a data frame\n```\n:::\n\n\n## Installation\n\n::: {.cell}\n\n```{.r .cell-code}\nremotes::install_github(\"xinzhuohkust/lacR\")\n```\n:::\n\n\n## Usage\n### setup\n\n::: {.cell}\n\n```{.r .cell-code}\nsetup_lac(custom = FALSE, location = NULL) # not use custom dictionary\n```\n:::\n\n### text segmentation\n\n::: {.cell}\n\n```{.r .cell-code}\ntokenizer(\n  string = \"我和梅东在上海市中山北路与华东政法大学师生共度一个春节\",\n  analysis = FALSE, # not to perform part-of-speech tagging\n  progress = TRUE, # display progress bar\n  min = 0 # keep all words\n)\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1] \"我\"           \"和\"           \"梅东\"         \"在\"           \"上海市\"      \n [6] \"中山北路\"     \"与\"           \"华东政法大学\" \"师生\"         \"共度\"        \n[11] \"一个\"         \"春节\"        \n```\n:::\n:::\n\n\n## Rcpp version (coming soon)",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}